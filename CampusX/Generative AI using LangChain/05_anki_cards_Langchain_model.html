<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LangChain Flashcards - 3D Introduction</title>
    <link rel="stylesheet" href="../../anki_cards_app/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>

<body>
    <div class="container">
        <div id="threejs-canvas-container">
            <!-- Three.js canvas will be appended here by the script -->
        </div>
    </div>

    <!-- Three.js Library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <!-- TWEEN.js for animations -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tween.js/18.6.4/tween.umd.min.js"></script>

    <!-- Core functionality -->
    <script src="../../anki_cards_app/card-core.js"></script>
    <!-- LangChain specific implementation -->
    <script src="../../anki_cards_app/langchain-cards.js"></script>

    <script>
        // Flashcard data
        const flashcards =  [
  {
    "front": "What is the primary purpose of the **Model component** within the LangChain framework?",
    "back": "To offer a **common interface**, simplifying interaction with a diverse range of AI models by enabling developers to connect and utilize them without extensive code adaptation for each model's specific API.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Core_Concept", "Model_Component", "Interface"],
    "mnemonic": "Think of LangChain Models as a 'Universal Adapter' for AI.",
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-1"
  },
  {
    "front": "LangChain's Model component provides a crucial {{c1::abstraction layer}} that simplifies interaction with various AI models.",
    "back": "LangChain's Model component provides a crucial abstraction layer that simplifies interaction with various AI models.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Component", "Abstraction"],
    "mnemonic": null,
    "examples": null,
    "cloze_back_extra": "LangChain's Model component provides a crucial abstraction layer that simplifies interaction with various AI models.",
    "id": "uuid-v4-2"
  },
  {
    "front": "What are the two fundamental categories of models distinguished within LangChain?",
    "back": "1. Language Models\n2. Embedding Models",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Types", "Categorization"],
    "mnemonic": "'LEt's remember: Language & Embedding.",
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-3"
  },
  {
    "front": "What is the characteristic input and output of **Language Models** in LangChain?",
    "back": "They process textual input and generate textual output.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Language_Models", "IO_Process"],
    "mnemonic": null,
    "examples": "A chatbot answering a user's question in text.",
    "cloze_back_extra": null,
    "id": "uuid-v4-4"
  },
  {
    "front": "What do **Embedding Models** in LangChain transform textual input into?",
    "back": "They transform textual input into **embeddings**, which are numerical (vector) representations of the text.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Embedding_Models", "Output", "Embeddings", "Vector_Representation"],
    "mnemonic": "Embedding models 'embed' meaning into numbers/vectors.",
    "examples": "Representing sentences as lists of numbers for similarity comparison.",
    "cloze_back_extra": null,
    "id": "uuid-v4-5"
  },
  {
    "front": "What key aspect of text do **embeddings** (generated by Embedding Models) capture?",
    "back": "Embeddings capture the **contextual meaning** or semantic information of the text.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Embedding_Models", "Embeddings", "Contextual_Meaning", "Semantics"],
    "mnemonic": null,
    "examples": "Understanding that 'apple pie recipe' and 'how to bake an apple dessert' are semantically similar.",
    "cloze_back_extra": null,
    "id": "uuid-v4-6"
  },
  {
    "front": "Embedding Models are vital for tasks like {{c1::semantic search}} and are a core element in building {{c2::RAG (Retrieval Augmented Generation)}} applications.",
    "back": "Embedding Models are vital for tasks like semantic search and are a core element in building RAG (Retrieval Augmented Generation) applications.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Embedding_Models", "Applications", "Semantic_Search", "RAG"],
    "mnemonic": null,
    "examples": "RAG: Using embeddings to find relevant document chunks to help an LLM answer a question.",
    "cloze_back_extra": "Embedding Models are vital for tasks like semantic search and are a core element in building RAG (Retrieval Augmented Generation) applications.",
    "id": "uuid-v4-7"
  },
  {
    "front": "In the LangChain context, what is the typical input/output of traditional **LLMs** (Large Language Models)?",
    "back": "They typically take a single string as input and return a single string as output.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Language_Models", "LLMs", "IO_Process"],
    "mnemonic": null,
    "examples": "Input: 'Translate this.' Output: 'Translated text.'",
    "cloze_back_extra": null,
    "id": "uuid-v4-8"
  },
  {
    "front": "Are traditional LLMs (string-in, string-out) considered the most modern approach in LangChain for new projects?",
    "back": "No, the material indicates they are becoming somewhat legacy, with a shift in recommendation towards Chat Models.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Language_Models", "LLMs", "Legacy"],
    "mnemonic": null,
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-9"
  },
  {
    "front": "What type of tasks are **Chat Models** in LangChain specialized for?",
    "back": "They are specialized for conversational tasks.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Language_Models", "Chat_Models", "Specialization"],
    "mnemonic": null,
    "examples": "Building interactive chatbots or virtual assistants.",
    "cloze_back_extra": null,
    "id": "uuid-v4-10"
  },
  {
    "front": "Chat Models in LangChain are designed to take a {{c1::sequence of messages}} as input and return {{c2::chat messages}} as output.",
    "back": "Chat Models in LangChain are designed to take a sequence of messages as input and return chat messages as output.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Chat_Models", "IO_Process", "Messages"],
    "mnemonic": null,
    "examples": "Input: [SystemMessage, HumanMessage, AIMessage], Output: AIMessage.",
    "cloze_back_extra": "Chat Models in LangChain are designed to take a sequence of messages as input and return chat messages as output.",
    "id": "uuid-v4-11"
  },
  {
    "front": "List two advantages of Chat Models over traditional LLMs mentioned in the material.",
    "back": "1. Support for **conversation history** (memory).\n2. Support for **role awareness** (assigning a persona to the AI).",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Chat_Models", "Advantages", "Conversation_History", "Role_Awareness"],
    "mnemonic": null,
    "examples": "Role awareness: 'You are a helpful pirate assistant.'",
    "cloze_back_extra": null,
    "id": "uuid-v4-12"
  },
  {
    "front": "What is the purpose of using `.env` files in LangChain projects, according to the material?",
    "back": "For secure management of API keys, keeping them separate from the codebase.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Development_Setup", "API_Keys", "Security", "DotEnv"],
    "mnemonic": "`.env` 'envelops' secrets.",
    "examples": "Storing `OPENAI_API_KEY` in a `.env` file rather than hardcoding it.",
    "cloze_back_extra": null,
    "id": "uuid-v4-13"
  },
  {
    "front": "Which common LangChain method is highlighted for interacting with models (LLMs or Chat Models) to get a response?",
    "back": "The **`invoke()`** method.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Interaction_Pattern", "Invoke_Method"],
    "mnemonic": null,
    "examples": "`model.invoke('What is LangChain?')`",
    "cloze_back_extra": null,
    "id": "uuid-v4-14"
  },
  {
    "front": "In LangChain, the output from Chat Models is often an {{c1::AIMessage object}}, and textual content is accessed via an attribute like {{c2::.content}}.",
    "back": "In LangChain, the output from Chat Models is often an AIMessage object, and textual content is accessed via an attribute like .content.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Chat_Models", "Output_Format", "AIMessage"],
    "mnemonic": null,
    "examples": "`response = chat_model.invoke(...); print(response.content)`",
    "cloze_back_extra": "In LangChain, the output from Chat Models is often an AIMessage object, and textual content is accessed via an attribute like .content.",
    "id": "uuid-v4-15"
  },
  {
    "front": "What does the **`temperature`** parameter control in language models?",
    "back": "It controls the creativity or randomness of the model's output. Lower values lead to more deterministic output, higher values to more random/creative output.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Parameters", "Temperature"],
    "mnemonic": "High temperature = 'hot' takes (more creative), Low temperature = 'cool' facts (more deterministic).",
    "examples": "Setting temperature to 0.0 for factual answers, 0.9 for story generation.",
    "cloze_back_extra": null,
    "id": "uuid-v4-16"
  },
  {
    "front": "What is the purpose of the **`max_tokens`** (or `max_new_tokens`) parameter in language models?",
    "back": "It limits the length of the output generated by the model, which can also help manage costs for API usage.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Parameters", "Max_Tokens", "Cost_Management"],
    "mnemonic": null,
    "examples": "Setting `max_tokens=50` to ensure a concise summary.",
    "cloze_back_extra": null,
    "id": "uuid-v4-17"
  },
  {
    "front": "**Closed-Source Models** (e.g., OpenAI's GPT) are typically accessed via {{c1::paid APIs}}.",
    "back": "Closed-Source Models (e.g., OpenAI's GPT) are typically accessed via paid APIs.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Types", "Closed_Source_Models", "Access_Method"],
    "mnemonic": null,
    "examples": "Using an API key to query GPT-4.",
    "cloze_back_extra": "Closed-Source Models (e.g., OpenAI's GPT) are typically accessed via paid APIs.",
    "id": "uuid-v4-18"
  },
  {
    "front": "What is one advantage of using Closed-Source Models mentioned in the material?",
    "back": "They are often highly capable, well-refined, and easier to get started with if API access is available.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Types", "Closed_Source_Models", "Pros"],
    "mnemonic": null,
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-19"
  },
  {
    "front": "What are two disadvantages of using Closed-Source Models listed in the material?",
    "back": "1. Costs associated with API usage.\n2. Data privacy concerns (data sent to third-party servers) / Limited control/customization.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Types", "Closed_Source_Models", "Cons"],
    "mnemonic": null,
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-20"
  },
  {
    "front": "**Open-Source Models** allow for full control, customization (including {{c1::fine-tuning}}), and enhanced {{c2::data privacy}} as data can stay on-premises.",
    "back": "Open-Source Models allow for full control, customization (including fine-tuning), and enhanced data privacy as data can stay on-premises.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Types", "Open_Source_Models", "Pros", "Fine_Tuning", "Data_Privacy"],
    "mnemonic": null,
    "examples": "Fine-tuning an open-source LLM on company-specific documents.",
    "cloze_back_extra": "Open-Source Models allow for full control, customization (including fine-tuning), and enhanced data privacy as data can stay on-premises.",
    "id": "uuid-v4-21"
  },
  {
    "front": "What is a significant drawback of using large Open-Source Models locally?",
    "back": "They can require significant **hardware resources**, especially GPUs.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Model_Types", "Open_Source_Models", "Cons", "Hardware_Requirements"],
    "mnemonic": null,
    "examples": "Needing a powerful graphics card to run a large Llama model.",
    "cloze_back_extra": null,
    "id": "uuid-v4-22"
  },
  {
    "front": "Which platform is identified as the primary repository for open-source AI models?",
    "back": "**Hugging Face**.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Open_Source_Models", "Hugging_Face", "Repository"],
    "mnemonic": "Hugging Face 🤗 gives open models a home.",
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-23"
  },
  {
    "front": "What are the two methods mentioned for using Hugging Face open-source models within LangChain?",
    "back": "1. Using the **Hugging Face Inference API** (e.g., with `ChatHuggingFace` and `HuggingFaceEndpoint`).\n2. **Local Download and Execution** (e.g., with `HuggingFacePipeline` or `HuggingFaceEmbeddings`).",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Open_Source_Models", "Hugging_Face", "Usage_Methods"],
    "mnemonic": null,
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-24"
  },
  {
    "front": "Which LangChain class is used for OpenAI's embedding models, offering methods like `embed_query` and `embed_documents`?",
    "back": "The **`OpenAIEmbeddings`** class.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Embedding_Models", "OpenAIEmbeddings"],
    "mnemonic": null,
    "examples": "`embeddings = OpenAIEmbeddings(model='text-embedding-3-large')`",
    "cloze_back_extra": null,
    "id": "uuid-v4-25"
  },
  {
    "front": "The `OpenAIEmbeddings` class allows specification of the embedding model and desired output {{c1::dimensions}}.",
    "back": "The `OpenAIEmbeddings` class allows specification of the embedding model and desired output dimensions.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Embedding_Models", "OpenAIEmbeddings", "Parameters", "Dimensions"],
    "mnemonic": null,
    "examples": "Requesting 1536-dimensional embeddings from an OpenAI model.",
    "cloze_back_extra": "The `OpenAIEmbeddings` class allows specification of the embedding model and desired output dimensions.",
    "id": "uuid-v4-26"
  },
  {
    "front": "Which LangChain class is used to leverage open-source sentence transformer models locally from Hugging Face for generating embeddings?",
    "back": "The **`HuggingFaceEmbeddings`** class.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Embedding_Models", "HuggingFaceEmbeddings", "Open_Source_Models"],
    "mnemonic": null,
    "examples": "`hf_embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')`",
    "cloze_back_extra": null,
    "id": "uuid-v4-27"
  },
  {
    "front": "What is the core idea of **Document Similarity Search** using embedding models?",
    "back": "To find which document in a collection is semantically closest (most similar in meaning) to a user's query.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Applications", "Semantic_Search", "Document_Similarity"],
    "mnemonic": null,
    "examples": "Finding articles related to 'AI ethics' by comparing their meaning to the query.",
    "cloze_back_extra": null,
    "id": "uuid-v4-28"
  },
  {
    "front": "In document similarity search, {{c1::cosine similarity}} is calculated between the query embedding and each document embedding to measure likeness.",
    "back": "In document similarity search, cosine similarity is calculated between the query embedding and each document embedding to measure likeness.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Applications", "Semantic_Search", "Cosine_Similarity", "Vector_Similarity"],
    "mnemonic": "Cosine 'aligns' with meaning similarity.",
    "examples": "A cosine similarity score of 0.9 indicates high semantic similarity between two texts.",
    "cloze_back_extra": "In document similarity search, cosine similarity is calculated between the query embedding and each document embedding to measure likeness.",
    "id": "uuid-v4-29"
  },
  {
    "front": "How is the process of document similarity search foundational to **RAG (Retrieval Augmented Generation)**?",
    "back": "In RAG, similar documents (or chunks) are first retrieved using techniques like document similarity search (via embeddings) to provide relevant context to an LLM for generating an informed answer.",
    "type": "basic",
    "tags": ["biology", "cellular_processes", "LangChain", "Applications", "RAG", "Semantic_Search", "Context_Retrieval"],
    "mnemonic": null,
    "examples": null,
    "cloze_back_extra": null,
    "id": "uuid-v4-30"
  },
  {
    "front": "For efficient retrieval in real-world RAG applications, document embeddings would ideally be stored in a {{c1::vector database}}.",
    "back": "For efficient retrieval in real-world RAG applications, document embeddings would ideally be stored in a vector database.",
    "type": "cloze",
    "tags": ["biology", "cellular_processes", "LangChain", "Applications", "RAG", "Vector_Database", "Embeddings_Storage"],
    "mnemonic": null,
    "examples": "Using Pinecone or Weaviate to store and query text embeddings.",
    "cloze_back_extra": "For efficient retrieval in real-world RAG applications, document embeddings would ideally be stored in a vector database.",
    "id": "uuid-v4-31"
  }
];

        // Initialize the card system
        document.addEventListener('DOMContentLoaded', () => {
            const cardSystem = new LangChainCards('threejs-canvas-container', {
                isDarkMode: window.matchMedia('(prefers-color-scheme: dark)').matches
            });
            cardSystem.loadDeck(flashcards);
        });
    </script>
</body>

</html>