<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LangChain Playlist Quiz</title>
    <link rel="stylesheet" href="../../static/quiz.css">
</head>

<body>
    <div class="quiz">
        <h2 id="quizTitle">LangChain Playlist Quiz</h2>
        <div id="quizContent">
            <!-- Questions will be dynamically inserted here -->
        </div>
        <button id="submitQuiz">Submit Quiz</button>
        <div id="quizScore" hidden>Your Score: 0 out of 0</div>
    </div>

    <!-- Quiz data -->
    <script class="quiz-json" type="application/json">
 
 {
  "quizTitle": "LangChain Framework Components Quiz",
  "settings": {
    "showHints": true,
    "mnemonics": false,
    "theme": "dark"
  },
  "questions": [
    {
      "id": "q1",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "What is the primary purpose of LangChain as described in the material?",
      "options": [
        {"value": "A", "text": "To provide direct access to LLM source code."},
        {"value": "B", "text": "To enable the building of applications leveraging Large Language Models (LLMs)."},
        {"value": "C", "text": "To replace the need for APIs when using LLMs."},
        {"value": "D", "text": "To create new Large Language Models from scratch."}
      ],
      "placeholder": null,
      "hint": "Think about what LangChain helps developers create.",
      "feedbackMessage": null
    },
    {
      "id": "q2",
      "type": "tf",
      "bloomLevel": "Understand",
      "text": "LangChain's model-agnostic nature means that switching between different LLM providers (e.g., OpenAI to Google) requires absolutely no code changes.",
      "options": [
        {"value": "True", "text": "True"},
        {"value": "False", "text": "False"}
      ],
      "placeholder": null,
      "hint": "Consider the degree of code modification LangChain aims for when switching models.",
      "feedbackMessage": "LangChain aims for minimal code changes (e.g., one or two lines), not necessarily zero changes."
    },
    {
      "id": "q3",
      "type": "gap",
      "bloomLevel": "Remember",
      "text": "The LangChain 'Models' component provides a ____ interface for interacting with various AI models.",
      "options": null,
      "placeholder": "Enter one word",
      "hint": "This component helps make interactions consistent across different model providers.",
      "feedbackMessage": "The word is 'standardized'."
    },
    {
      "id": "q4",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "Which of these is NOT a problem that LLM APIs helped to solve initially, according to the material?",
      "options": [
        {"value": "A", "text": "Large size of LLM files making them hard to host."},
        {"value": "B", "text": "Difficulty for individuals/small companies to run LLMs."},
        {"value": "C", "text": "The need to pay per use for LLM access."},
        {"value": "D", "text": "Varied API implementations across different LLM providers."}
      ],
      "placeholder": null,
      "hint": "Three options are problems LLM APIs addressed. One option is a problem that arose *because* of different APIs.",
      "feedbackMessage": "Varied API implementations (D) was a new problem that LangChain's Model component then addressed. The pay-per-use model (C) was a feature of API access."
    },
    {
      "id": "q5",
      "type": "mcq",
      "bloomLevel": "Understand",
      "text": "What are the two main categories of AI models that LangChain's 'Models' component provides interfaces for?",
      "options": [
        {"value": "A", "text": "Predictive Models and Generative Models"},
        {"value": "B", "text": "Language Models and Embedding Models"},
        {"value": "C", "text": "Classification Models and Regression Models"},
        {"value": "D", "text": "Speech Models and Vision Models"}
      ],
      "placeholder": null,
      "hint": "One type is for text-in, text-out operations, and the other is for text-in, vector-out.",
      "feedbackMessage": null
    },
    {
      "id": "q6",
      "type": "short",
      "bloomLevel": "Apply",
      "text": "Explain how LangChain's 'Models' component benefits a developer who wants to experiment with LLMs from both OpenAI and Anthropic in their application.",
      "options": null,
      "placeholder": "Your explanation...",
      "hint": "Focus on the standardization aspect and reduced code changes.",
      "feedbackMessage": "A good explanation would mention that the developer can use a unified interface for both, requiring minimal code changes to switch or use them concurrently, thus saving time and effort in learning and implementing different API specifications."
    },
    {
      "id": "q7",
      "type": "gap",
      "bloomLevel": "Remember",
      "text": "In the context of LLMs, 'Prompts' are the ____ provided to the LLM.",
      "options": null,
      "placeholder": "Enter one word",
      "hint": "This is what you give to the LLM to get a response.",
      "feedbackMessage": "The word is 'inputs'."
    },
    {
      "id": "q8",
      "type": "mcq",
      "bloomLevel": "Understand",
      "text": "Why is 'Prompt Engineering' considered an important field in the world of LLMs?",
      "options": [
        {"value": "A", "text": "Because LLMs can only understand prompts written in specific programming languages."},
        {"value": "B", "text": "Because LLM outputs are highly sensitive to the phrasing and structure of the prompts."},
        {"value": "C", "text": "Because it helps in reducing the computational cost of running LLMs."},
        {"value": "D", "text": "Because prompts are the only way to train new LLMs."}
      ],
      "placeholder": null,
      "hint": "Think about the relationship between the input to an LLM and its output.",
      "feedbackMessage": null
    },
    {
      "id": "q9",
      "type": "tf",
      "bloomLevel": "Apply",
      "text": "Using LangChain's 'Role-Based Prompts' by setting a system-level prompt like 'You are a witty poet' is an example of dynamic and reusable prompting.",
      "options": [
        {"value": "True", "text": "True"},
        {"value": "False", "text": "False"}
      ],
      "placeholder": null,
      "hint": "Consider if defining a persona for the LLM fits into making prompts adaptable.",
      "feedbackMessage": "This technique indeed guides the LLM's response style, making it a form of structured and potentially reusable prompting."
    },
    {
      "id": "q10",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "What is the core function of 'Chains' in LangChain?",
      "options": [
        {"value": "A", "text": "To secure the communication between the user and the LLM."},
        {"value": "B", "text": "To build pipelines or sequences of operations, linking multiple calls."},
        {"value": "C", "text": "To store the memory of past conversations with the LLM."},
        {"value": "D", "text": "To select the best LLM model for a given task automatically."}
      ],
      "placeholder": null,
      "hint": "The name 'LangChain' itself hints at this component's primary role.",
      "feedbackMessage": null
    },
    {
      "id": "q11",
      "type": "understand",
      "bloomLevel": "Understand",
      "text": "Explain the key characteristic of data flow within a LangChain 'Chain'.",
      "options": null,
      "placeholder": "Your explanation...",
      "hint": "How does the output of one step relate to the input of the next step in a chain?",
      "feedbackMessage": "The output of one component in a chain automatically becomes the input for the subsequent component, without requiring manual coding for this linkage."
    },
    {
      "id": "q12",
      "type": "mcq",
      "bloomLevel": "Analyse",
      "text": "What is a key difference between a 'Sequential Chain' and a 'Parallel Chain' in LangChain?",
      "options": [
        {"value": "A", "text": "Sequential chains use multiple LLMs, while parallel chains use only one."},
        {"value": "B", "text": "Sequential chains execute operations one after another, while parallel chains can execute multiple operations concurrently."},
        {"value": "C", "text": "Sequential chains are for text processing, while parallel chains are for image processing."},
        {"value": "D", "text": "Sequential chains require memory, while parallel chains do not."}
      ],
      "placeholder": null,
      "hint": "Think about the order and simultaneity of operations.",
      "feedbackMessage": null
    },
    {
      "id": "q13",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "What is the main purpose of the 'Indexes' component in LangChain?",
      "options": [
        {"value": "A", "text": "To create an index of all available LLM models."},
        {"value": "B", "text": "To connect LLM applications to external knowledge sources like PDFs or databases."},
        {"value": "C", "text": "To speed up the processing time of LLM API calls."},
        {"value": "D", "text": "To index the internal parameters of an LLM for fine-tuning."}
      ],
      "placeholder": null,
      "hint": "This component helps LLMs answer questions about data they weren't trained on.",
      "feedbackMessage": null
    },
    {
      "id": "q14",
      "type": "gap",
      "bloomLevel": "Understand",
      "text": "The process of enabling LLMs to use external data sources for answering questions, often facilitated by LangChain's 'Indexes', is known as ____ ____ ____ (RAG).",
      "options": null,
      "placeholder": "Enter three words",
      "hint": "This acronym stands for a common pattern in LLM applications.",
      "feedbackMessage": "The words are 'Retrieval Augmented Generation'."
    },
    {
      "id": "q15",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "Which sub-component of LangChain 'Indexes' is responsible for loading data from sources like PDFs or websites?",
      "options": [
        {"value": "A", "text": "Text Splitters"},
        {"value": "B", "text": "Vector Stores"},
        {"value": "C", "text": "Document Loaders"},
        {"value": "D", "text": "Retrievers"}
      ],
      "placeholder": null,
      "hint": "This is the first step in making external data available.",
      "feedbackMessage": null
    },
    {
      "id": "q16",
      "type": "short",
      "bloomLevel": "Apply",
      "text": "Imagine you want to build an LLM application that answers questions based on your company's 500-page internal policy manual (a single PDF). Briefly describe how the 'Text Splitters' and 'Vector Stores' sub-components of LangChain's 'Indexes' would be used.",
      "options": null,
      "placeholder": "Your description...",
      "hint": "Think about how a large document is processed for efficient search.",
      "feedbackMessage": "Text Splitters would break the 500-page manual into smaller, manageable chunks. Vector Stores would then store the numerical embeddings (vector representations) of these chunks, allowing for semantic search later."
    },
    {
      "id": "q17",
      "type": "tf",
      "bloomLevel": "Analyse",
      "text": "In LangChain's 'Indexes', 'Retrievers' perform semantic search by directly comparing the text of the user's query with the text of all chunks in the external document.",
      "options": [
        {"value": "True", "text": "True"},
        {"value": "False", "text": "False"}
      ],
      "placeholder": null,
      "hint": "Consider what 'semantic search' implies and how 'Vector Stores' are used.",
      "feedbackMessage": "False. Retrievers generate an embedding for the user's query and compare this embedding with the stored embeddings of the text chunks in the vector store to find semantically similar content."
    },
    {
      "id": "q18",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "What fundamental problem with LLM API calls does the 'Memory' component in LangChain aim to solve?",
      "options": [
        {"value": "A", "text": "Their high financial cost."},
        {"value": "B", "text": "Their stateless nature (lack of recall of past interactions)."},
        {"value": "C", "text": "Their slow response times."},
        {"value": "D", "text": "Their inability to understand complex prompts."}
      ],
      "placeholder": null,
      "hint": "Think about what is missing in a typical API call for a continuous conversation.",
      "feedbackMessage": null
    },
    {
      "id": "q19",
      "type": "understand",
      "bloomLevel": "Understand",
      "text": "Why would a chatbot built without a 'Memory' component struggle if a user asks 'Who is the current prime minister?' and then follows up with 'How old is he?'?",
      "options": null,
      "placeholder": "Your explanation...",
      "hint": "Relate this to the stateless nature of LLM API calls.",
      "feedbackMessage": "Because each API call is stateless, the LLM wouldn't remember that the previous question was about the prime minister. Thus, it wouldn't understand who 'he' refers to in the follow-up question without the context provided by a memory component."
    },
    {
      "id": "q20",
      "type": "mcq",
      "bloomLevel": "Analyse",
      "text": "Which type of LangChain 'Memory' stores only the last 'N' interactions to manage context size while still providing recent history?",
      "options": [
        {"value": "A", "text": "ConversationBufferMemory"},
        {"value": "B", "text": "Summarizer-Based Memory"},
        {"value": "C", "text": "ConversationBufferWindowMemory"},
        {"value": "D", "text": "Custom Memory"}
      ],
      "placeholder": null,
      "hint": "The name of this memory type suggests a limited scope of recall.",
      "feedbackMessage": null
    },
    {
      "id": "q21",
      "type": "mcq",
      "bloomLevel": "Remember",
      "text": "According to the material, what are the two key capabilities that distinguish 'AI Agents' in LangChain from simpler chatbots?",
      "options": [
        {"value": "A", "text": "Faster response times and multilingual support."},
        {"value": "B", "text": "Enhanced NLU and more natural text generation."},
        {"value": "C", "text": "Reasoning capability and access to tools."},
        {"value": "D", "text": "Built-in memory and ability to use multiple LLMs."}
      ],
      "placeholder": null,
      "hint": "These capabilities allow agents to do more than just talk.",
      "feedbackMessage": null
    },
    {
      "id": "q22",
      "type": "tf",
      "bloomLevel": "Understand",
      "text": "An AI Agent in LangChain can decide which external tool to use (e.g., a calculator or a weather API) based on the user's query.",
      "options": [
        {"value": "True", "text": "True"},
        {"value": "False", "text": "False"}
      ],
      "placeholder": null,
      "hint": "Consider the agent's reasoning capability and its interaction with tools.",
      "feedbackMessage": "True, this is a key aspect of how AI Agents function; they can reason about the query and select appropriate tools."
    },
    {
      "id": "q23",
      "type": "short",
      "bloomLevel": "Apply",
      "text": "Briefly explain how an AI Agent in LangChain might use 'Chain of Thought (CoT) prompting' to handle a complex user request like 'Book the cheapest flight to London next Friday and then find a hotel near the airport.'",
      "options": null,
      "placeholder": "Your explanation...",
      "hint": "Focus on how CoT helps in breaking down the problem and planning steps.",
      "feedbackMessage": "CoT prompting would allow the agent to break down the request into sequential steps: 1. Find cheapest flight to London for next Friday (tool: flight API). 2. Once flight is found/booked, identify the arrival airport. 3. Find a hotel near that airport (tool: hotel API/search). This step-by-step reasoning helps the agent plan and execute actions using appropriate tools."
    },
    {
      "id": "q24",
      "type": "evaluate",
      "bloomLevel": "Evaluate",
      "text": "Consider a scenario where a company wants to build a customer support chatbot using LangChain. Evaluate the trade-offs between using 'ConversationBufferMemory' and 'Summarizer-Based Memory' for this chatbot.",
      "options": null,
      "placeholder": "Your evaluation...",
      "hint": "Think about context accuracy, API costs, and complexity for long conversations.",
      "feedbackMessage": "Model answer evaluation guidelines: A good evaluation would discuss that ConversationBufferMemory provides full context accuracy but can become very expensive with long chats due to sending the entire history. Summarizer-Based Memory reduces API costs by sending a condensed summary, but might lose some conversational nuance or specific details, potentially affecting the quality of long-tail support interactions. The choice depends on balancing cost with the required level of conversational fidelity."
    },
    {
      "id": "q25",
      "type": "create",
      "bloomLevel": "Create",
      "text": "Design a concept for a simple LangChain application that uses at least 'Models', 'Prompts', and 'Chains'. Briefly describe its purpose and how these three components would interact.",
      "options": null,
      "placeholder": "Your application concept...",
      "hint": "Think of a multi-step process. For example, content generation, translation, or data transformation.",
      "feedbackMessage": "Model answer evaluation guidelines: An example could be an 'Automatic Blog Post Idea Generator & Outliner'. Purpose: User provides a topic, application generates 3 blog post ideas and a brief outline for the chosen idea. Interaction: 1. Prompts: A prompt template to ask the Model for blog ideas based on {user_topic}. Another prompt template to ask the Model for an outline for a {chosen_idea}. 2. Models: An LLM to generate ideas and then an outline. 3. Chains: A sequential chain where the first step takes the user topic, uses the first prompt and model to generate ideas. The second step (potentially after user input) takes the chosen idea, uses the second prompt and model to generate the outline."
    },
    {
      "id": "q26",
      "type": "evaluate",
      "bloomLevel": "Evaluate",
      "text": "LangChain's 'Indexes' component allows LLMs to access external knowledge. How does this capability address the common LLM limitation of 'hallucination' (generating plausible but incorrect or nonsensical information)?",
      "options": null,
      "placeholder": "Your explanation...",
      "hint": "Consider how grounding responses in factual, external data impacts an LLM's output.",
      "feedbackMessage": "Model answer evaluation guidelines: By providing the LLM with relevant, factual information retrieved from specific external documents (via RAG), the LLM's responses can be grounded in this context. This reduces the likelihood of hallucination because the LLM is guided to formulate answers based on the provided data rather than relying solely on its internal, potentially flawed or outdated, training data."
    },
    {
      "id": "q27",
      "type": "create",
      "bloomLevel": "Create",
      "text": "Imagine you are tasked with creating an 'AI Agent' using LangChain that helps users plan a healthy meal. What are two distinct 'Tools' you would equip this agent with, and briefly describe a scenario where the agent would use its 'Reasoning Capability' to decide which tool to use.",
      "options": null,
      "placeholder": "Your tool descriptions and scenario...",
      "hint": "Think about what information or actions are needed for meal planning (e.g., nutritional info, recipes).",
      "feedbackMessage": "Model answer evaluation guidelines: Example Tools: 1. Nutritional Database API (provides calorie, macro/micro-nutrient info for foods). 2. Recipe Search API (finds recipes based on ingredients or dietary needs). Scenario: User says, 'I want a low-carb dinner with chicken.' Reasoning: The agent would reason it first needs recipe ideas (use Recipe Search API with 'low-carb', 'chicken', 'dinner'). Once a recipe is suggested, if the user asks 'How many calories are in this?', the agent would reason it needs nutritional data for the ingredients (use Nutritional Database API for each ingredient in the chosen recipe)."
    },
    {
      "id": "q28",
      "type": "analyse",
      "bloomLevel": "Analyse",
      "text": "The material states LLMs are large (>100GB) and led to API-based access. How does the LangChain 'Models' component, by standardizing API interactions, indirectly support the broader adoption of diverse LLMs despite their individual complexities?",
      "options": null,
      "placeholder": "Your analysis...",
      "hint": "Consider the barrier to entry for developers if they had to learn many different API styles.",
      "feedbackMessage": "By providing a consistent interface, LangChain lowers the barrier for developers to integrate and switch between various LLMs. Without this standardization, the effort required to learn and implement each provider's unique API would be significant, potentially discouraging the use of diverse models and favoring only the most well-known or easiest-to-use APIs. Standardization encourages experimentation and use of the best model for a specific task, regardless of its provider."
    },
    {
      "id": "q29",
      "type": "evaluate",
      "bloomLevel": "Evaluate",
      "text": "Assess the claim: 'For building a simple Q&A chatbot over a small, static FAQ document, using the full suite of LangChain's 'Indexes' (Document Loaders, Text Splitters, Vector Stores, Retrievers) is overkill.' Justify your position.",
      "options": null,
      "placeholder": "Your assessment...",
      "hint": "Consider the benefits of semantic search even for small documents versus simpler methods.",
      "feedbackMessage": "Model answer evaluation guidelines: One might argue it's overkill if the FAQ is extremely small and exact keyword matching suffices. However, even for small documents, semantic search (enabled by Indexes) can handle variations in user queries better than simple keyword matching, leading to a more robust Q&A experience. The setup effort for LangChain Indexes might be relatively low, providing future scalability. The alternative (e.g., hardcoding Q&A pairs) is less flexible. So, while simpler methods exist, 'overkill' depends on desired robustness and future-proofing. For high-quality semantic Q&A, it's likely beneficial."
    },
    {
      "id": "q30",
      "type": "create",
      "bloomLevel": "Create",
      "text": "Propose a specific type of 'Custom Memory' for a LangChain-powered e-commerce chatbot. Describe what information this custom memory would store and how it would enhance the user's shopping experience beyond what standard conversation history provides.",
      "options": null,
      "placeholder": "Your custom memory proposal...",
      "hint": "Think about user preferences, past purchases, or Browse habits specific to e-commerce.",
      "feedbackMessage": "Model answer evaluation guidelines: Example Custom Memory: 'UserProductPreferenceMemory'. Information Stored: Recently viewed product categories, preferred brands, sizes (if applicable from past interactions/purchases), items added to wishlist or cart but not purchased. Enhancement: Beyond remembering the last few messages, this memory allows the chatbot to proactively suggest relevant products ('I see you liked X brand before, we have new arrivals from them!'), offer personalized discounts on items in their wishlist, or remind them of items left in their cart, making the interaction more targeted and helpful than generic conversation history."
    }
  ],
  "answers": {
    "q1": "B",
    "q2": "False",
    "q3": "standardized",
    "q4": "D",
    "q5": "B",
    "q6_evaluation": {
      "type": "model_answer",
      "modelAnswer": "A good explanation would mention that the developer can use a unified interface for both, requiring minimal code changes to switch or use them concurrently, thus saving time and effort in learning and implementing different API specifications."
    },
    "q7": "inputs",
    "q8": "B",
    "q9": "True",
    "q10": "B",
    "q11_evaluation": {
      "type": "model_answer",
      "modelAnswer": "The output of one component in a chain automatically becomes the input for the subsequent component, without requiring manual coding for this linkage."
    },
    "q12": "B",
    "q13": "B",
    "q14": "Retrieval Augmented Generation",
    "q15": "C",
    "q16_evaluation": {
      "type": "model_answer",
      "modelAnswer": "Text Splitters would break the 500-page manual into smaller, manageable chunks. Vector Stores would then store the numerical embeddings (vector representations) of these chunks, allowing for semantic search later."
    },
    "q17": "False",
    "q18": "B",
    "q19_evaluation": {
      "type": "model_answer",
      "modelAnswer": "Because each API call is stateless, the LLM wouldn't remember that the previous question was about the prime minister. Thus, it wouldn't understand who 'he' refers to in the follow-up question without the context provided by a memory component."
    },
    "q20": "C",
    "q21": "C",
    "q22": "True",
    "q23_evaluation": {
      "type": "model_answer",
      "modelAnswer": "CoT prompting would allow the agent to break down the request into sequential steps: 1. Find cheapest flight to London for next Friday (tool: flight API). 2. Once flight is found/booked, identify the arrival airport. 3. Find a hotel near that airport (tool: hotel API/search). This step-by-step reasoning helps the agent plan and execute actions using appropriate tools."
    },
    "q24_evaluation": {
      "type": "self-assess",
      "modelAnswer": "A good evaluation would discuss that ConversationBufferMemory provides full context accuracy but can become very expensive with long chats due to sending the entire history. Summarizer-Based Memory reduces API costs by sending a condensed summary, but might lose some conversational nuance or specific details, potentially affecting the quality of long-tail support interactions. The choice depends on balancing cost with the required level of conversational fidelity."
    },
    "q25_evaluation": {
      "type": "self-assess",
      "modelAnswer": "An example could be an 'Automatic Blog Post Idea Generator & Outliner'. Purpose: User provides a topic, application generates 3 blog post ideas and a brief outline for the chosen idea. Interaction: 1. Prompts: A prompt template to ask the Model for blog ideas based on {user_topic}. Another prompt template to ask the Model for an outline for a {chosen_idea}. 2. Models: An LLM to generate ideas and then an outline. 3. Chains: A sequential chain where the first step takes the user topic, uses the first prompt and model to generate ideas. The second step (potentially after user input) takes the chosen idea, uses the second prompt and model to generate the outline."
    },
    "q26_evaluation": {
        "type": "self-assess",
        "modelAnswer": "By providing the LLM with relevant, factual information retrieved from specific external documents (via RAG), the LLM's responses can be grounded in this context. This reduces the likelihood of hallucination because the LLM is guided to formulate answers based on the provided data rather than relying solely on its internal, potentially flawed or outdated, training data."
    },
    "q27_evaluation": {
        "type": "self-assess",
        "modelAnswer": "Example Tools: 1. Nutritional Database API (provides calorie, macro/micro-nutrient info for foods). 2. Recipe Search API (finds recipes based on ingredients or dietary needs). Scenario: User says, 'I want a low-carb dinner with chicken.' Reasoning: The agent would reason it first needs recipe ideas (use Recipe Search API with 'low-carb', 'chicken', 'dinner'). Once a recipe is suggested, if the user asks 'How many calories are in this?', the agent would reason it needs nutritional data for the ingredients (use Nutritional Database API for each ingredient in the chosen recipe)."
    },
    "q28_evaluation": {
        "type": "self-assess",
        "modelAnswer": "By providing a consistent interface, LangChain lowers the barrier for developers to integrate and switch between various LLMs. Without this standardization, the effort required to learn and implement each provider's unique API would be significant, potentially discouraging the use of diverse models and favoring only the most well-known or easiest-to-use APIs. Standardization encourages experimentation and use of the best model for a specific task, regardless of its provider."
    },
    "q29_evaluation": {
        "type": "self-assess",
        "modelAnswer": "One might argue it's overkill if the FAQ is extremely small and exact keyword matching suffices. However, even for small documents, semantic search (enabled by Indexes) can handle variations in user queries better than simple keyword matching, leading to a more robust Q&A experience. The setup effort for LangChain Indexes might be relatively low, providing future scalability. The alternative (e.g., hardcoding Q&A pairs) is less flexible. So, while simpler methods exist, 'overkill' depends on desired robustness and future-proofing. For high-quality semantic Q&A, it's likely beneficial."
    },
    "q30_evaluation": {
        "type": "self-assess",
        "modelAnswer": "Example Custom Memory: 'UserProductPreferenceMemory'. Information Stored: Recently viewed product categories, preferred brands, sizes (if applicable from past interactions/purchases), items added to wishlist or cart but not purchased. Enhancement: Beyond remembering the last few messages, this memory allows the chatbot to proactively suggest relevant products ('I see you liked X brand before, we have new arrivals from them!'), offer personalized discounts on items in their wishlist, or remind them of items left in their cart, making the interaction more targeted and helpful than generic conversation history."
    }
  }
}
    </script>
    <script src="../../static/quiz.js"></script>
</body>

</html>