# NLP Pipeline | Complete End-to-End Workflow | NLP Lecture 2

## Core Concept: The 5-Step NLP Pipeline

The **NLP Pipeline** represents a systematic approach to building any NLP software solution. This comprehensive workflow ensures that raw text data is transformed into actionable insights through a structured series of steps.

### **The Five Essential Steps**

1. **Data Acquisition** - Obtaining the necessary text data
2. **Text Preprocessing** - Cleaning and preparing the data
3. **Feature Engineering** - Converting text to numerical representations
4. **Modeling** - Applying algorithms and evaluating performance
5. **Deployment** - Making the solution available to users

> **Key Insight**: This pipeline is iterative, not linear. You may need to revisit earlier steps based on results from later stages.

### **Pipeline Visual Overview**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Acquisitionâ”‚ â”€â”€> â”‚Text Preprocessingâ”‚ â”€â”€> â”‚Feature Engineer.â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                                                  â”‚
         â”‚                                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Deployment    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    Modeling     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step 1: Data Acquisition

### **Three Primary Scenarios**

#### **Scenario A: Internal Data Available**
- **Case 1**: Data readily available on your desk (CSV files, databases)
  - **Action**: Proceed directly to preprocessing
- **Case 2**: Data exists in company databases
  - **Action**: Collaborate with data engineering team
  - **Requirement**: Communication skills and cross-team coordination
- **Case 3**: Insufficient data volume
  - **Solution**: **Data Augmentation Techniques**

#### **Data Augmentation Strategies**
When you have limited data, several techniques can help generate additional training examples:

1. **Synonym Replacement**
   - Replace words with their synonyms
   - Example: "red" â†’ "crimson", "running" â†’ "jogging"
   - **Libraries**: NLTK WordNet, spaCy

2. **N-gram Permutation** 
   - Rearrange word order while preserving meaning
   - Example: "walking on the road" â†’ "on the road walking"
   - **Caution**: Maintain grammatical correctness

3. **Back Translation**
   - Translate text to another language, then back to original
   - Introduces natural variation while preserving meaning
   - **Tools**: Google Translate API, MarianMT

4. **Noise Injection**
   - Add random words that don't change sentiment
   - Simulates real-world text variations
   - **Types**: Character-level noise, word deletion, word insertion

5. **Paraphrasing**
   - Generate semantically similar sentences
   - **Tools**: T5, PEGASUS, GPT-based paraphrasing

6. **Contextual Word Embeddings**
   - Use BERT/GPT to generate contextually similar variations
   - Maintains semantic coherence better than random replacements

#### **Scenario B: External Data Available**
Multiple approaches for acquiring external data:

1. **Public Datasets**
   - Platforms: Kaggle, Google Dataset Search, University repositories
   - **Limitation**: May not match your specific domain

2. **Web Scraping**
   - Tools: Beautiful Soup, Scrapy
   - **Challenges**: HTML structure complexity, unwanted content

3. **API Integration**
   - Platforms: RapidAPI.com for various data sources
   - **Advantage**: Structured JSON format
   - **Requirement**: API key and rate limit management

4. **Document Processing**
   - **PDF Extraction**: PyPDF2, pdfplumber libraries
   - **Image Text Extraction**: OCR tools (Tesseract)
   - **Audio Transcription**: Speech-to-text services

#### **Scenario C: No Available Data**
When you're pioneering a new application:

1. **User-Generated Content**
   - Deploy forms to collect initial data from users
   - Manual labeling required initially

2. **Rule-Based Bootstrap**
   - Create heuristic-based system first
   - Use regular expressions and keyword matching
   - Gradually transition to ML as data grows

---

## Step 2: Text Preprocessing

### **Three Levels of Preprocessing**

#### **Level 1: Basic Cleaning**

**HTML Tag Removal**
```python
import re
from bs4 import BeautifulSoup

# Method 1: Regular Expression (Simple)
def remove_html_tags(text):
    pattern = r'<[^>]+>'
    return re.sub(pattern, '', text)

# Method 2: BeautifulSoup (More Robust)
def remove_html_tags_soup(text):
    soup = BeautifulSoup(text, 'html.parser')
    return soup.get_text(separator=' ', strip=True)

# Example
input_text = "<p>Hello <b>World</b></p>"
clean_text = remove_html_tags(input_text)  # "Hello World"
clean_text_soup = remove_html_tags_soup(input_text)  # "Hello World"
```

**Unicode Normalization (Emoji Handling)**
```python
import unicodedata
import emoji

# Method 1: Remove all non-ASCII characters
def normalize_unicode(text):
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')

# Method 2: Convert emojis to text description
def demojize_text(text):
    return emoji.demojize(text)

# Method 3: Remove emojis entirely
def remove_emojis(text):
    return emoji.replace_emoji(text, replace='')

# Examples
text_with_emoji = "I love Python ğŸ programming! ğŸ˜Š"
print(normalize_unicode(text_with_emoji))  # "I love Python  programming! "
print(demojize_text(text_with_emoji))      # "I love Python :snake: programming! :smiling_face_with_smiling_eyes:"
print(remove_emojis(text_with_emoji))      # "I love Python  programming! "
```

**Spelling Correction**
```python
from textblob import TextBlob
import spellchecker
from autocorrect import Speller

# Method 1: TextBlob (Simple but effective)
def correct_spelling_textblob(text):
    return str(TextBlob(text).correct())

# Method 2: PySpellChecker (More control)
def correct_spelling_pyspell(text):
    spell = spellchecker.SpellChecker()
    words = text.split()
    corrected = []
    for word in words:
        correction = spell.correction(word)
        corrected.append(correction if correction else word)
    return ' '.join(corrected)

# Method 3: Autocorrect (Fast)
def correct_spelling_auto(text):
    spell = Speller(lang='en')
    return spell(text)

# Example
misspelled = "I havv a problm with speling"
print(correct_spelling_textblob(misspelled))  # "I have a problem with spelling"
```

#### **Level 2: Basic Preprocessing (Fundamental)**

**Tokenization**
- **Sentence Tokenization**: Splitting paragraphs into sentences
- **Word Tokenization**: Breaking sentences into individual words
- **Subword Tokenization**: Breaking words into smaller units (for modern NLP)

```python
import nltk
import spacy
from transformers import AutoTokenizer

# Method 1: NLTK Tokenization
def nltk_tokenize(text):
    # Download required data: nltk.download('punkt')
    sentences = nltk.sent_tokenize(text)
    words = [nltk.word_tokenize(sent) for sent in sentences]
    return sentences, words

# Method 2: spaCy Tokenization (More accurate)
def spacy_tokenize(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]
    words = [token.text for token in doc]
    return sentences, words

# Method 3: Subword Tokenization (BERT-style)
def bert_tokenize(text):
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    tokens = tokenizer.tokenize(text)
    token_ids = tokenizer.encode(text)
    return tokens, token_ids

# Example
text = "Dr. Smith went to New York. He'll return tomorrow."
sentences, words = nltk_tokenize(text)
print(f"Sentences: {sentences}")
print(f"Words: {words}")
```

#### **Level 3: Optional Preprocessing**

**Stop Word Removal**
- Removes common words that don't contribute to meaning
- Examples: "the", "is", "at", "and"
- **Context Dependency**: Sometimes important for syntax analysis

```python
import nltk
from nltk.corpus import stopwords
import spacy

# Method 1: NLTK Stop Words
def remove_stopwords_nltk(text):
    # Download: nltk.download('stopwords')
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text.lower())
    filtered = [w for w in words if w not in stop_words]
    return ' '.join(filtered)

# Method 2: spaCy Stop Words
def remove_stopwords_spacy(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text.lower())
    filtered = [token.text for token in doc if not token.is_stop]
    return ' '.join(filtered)

# Method 3: Custom Stop Words
def remove_custom_stopwords(text, custom_stops):
    words = text.lower().split()
    filtered = [w for w in words if w not in custom_stops]
    return ' '.join(filtered)

# Example
text = "The quick brown fox jumps over the lazy dog"
print(remove_stopwords_nltk(text))  # "quick brown fox jumps lazy dog"
```

**Stemming vs Lemmatization**
- **Stemming**: Reduces words to root form (dance, dancing, danced â†’ danc)
- **Lemmatization**: Converts to dictionary form (better, best â†’ good)

```python
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
import spacy

# Stemming Examples
def stem_text(text):
    # Porter Stemmer (Most common)
    porter = PorterStemmer()
    
    # Snowball Stemmer (More aggressive)
    snowball = SnowballStemmer('english')
    
    words = text.split()
    porter_stemmed = [porter.stem(w) for w in words]
    snowball_stemmed = [snowball.stem(w) for w in words]
    
    return porter_stemmed, snowball_stemmed

# Lemmatization Examples
def lemmatize_text(text):
    # NLTK Lemmatizer
    lemmatizer = WordNetLemmatizer()
    words = text.split()
    
    # Basic lemmatization
    lemmatized = [lemmatizer.lemmatize(w) for w in words]
    
    # With POS tags for better accuracy
    lemmatized_verb = [lemmatizer.lemmatize(w, pos='v') for w in words]
    
    return lemmatized, lemmatized_verb

# spaCy Lemmatization (More accurate)
def lemmatize_spacy(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    return [token.lemma_ for token in doc]

# Example comparison
text = "running runs ran better best"
porter, snowball = stem_text(text)
lemma_basic, lemma_verb = lemmatize_text(text)
lemma_spacy = lemmatize_spacy(text)

print(f"Original: {text}")
print(f"Porter Stemming: {porter}")
print(f"Lemmatization: {lemma_spacy}")
```

**Case Normalization**
- Converts all text to lowercase for consistency

**Punctuation and Digit Removal**
- Application-dependent preprocessing step

**Language Detection**
- Essential for multilingual applications
- Libraries: langdetect, polyglot

### **Advanced Preprocessing**

**Part-of-Speech (POS) Tagging**
- Labels each word with its grammatical role
- Example: "I am living in Gurgaon"
  - I (Pronoun), am (Verb), living (Verb), in (Preposition), Gurgaon (Noun)

```python
import nltk
import spacy

# Method 1: NLTK POS Tagging
def pos_tag_nltk(text):
    # Download: nltk.download('averaged_perceptron_tagger')
    tokens = nltk.word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    return pos_tags

# Method 2: spaCy POS Tagging (More detailed)
def pos_tag_spacy(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    pos_tags = [(token.text, token.pos_, token.tag_) for token in doc]
    return pos_tags

# Universal POS Tags mapping
POS_TAGS = {
    'NOUN': 'Noun',
    'VERB': 'Verb', 
    'ADJ': 'Adjective',
    'ADV': 'Adverb',
    'PRON': 'Pronoun',
    'DET': 'Determiner',
    'PREP': 'Preposition',
    'NUM': 'Number',
    'CONJ': 'Conjunction',
    'INTJ': 'Interjection'
}

# Example
text = "The beautiful cat quickly jumped over the fence"
print("NLTK POS Tags:", pos_tag_nltk(text))
print("\nspaCy POS Tags:", pos_tag_spacy(text))
```

**Parsing**
- Analyzes sentence structure and relationships
- Creates syntactic trees for grammatical understanding

**Coreference Resolution**
- Resolves pronoun references
- Example: "Hitesh is a developer. He works at Google."
  - Links "He" â†’ "Hitesh"

```python
# Using AllenNLP (Previously) or Hugging Face
import spacy
import neuralcoref  # Note: Deprecated but conceptually important

# Modern approach using Hugging Face
from transformers import pipeline

def resolve_coreferences(text):
    # Load coreference resolution pipeline
    coref = pipeline("coreference-resolution", 
                     model="AllenAI/longformer-coreference")
    
    # Resolve coreferences
    result = coref(text)
    
    # Extract resolved text
    resolved_text = result['resolved_text']
    clusters = result['clusters']
    
    return resolved_text, clusters

# Example implementation for simple cases
def simple_coref_resolution(text):
    """
    Simple rule-based coreference resolution
    """
    sentences = text.split('. ')
    entities = {}  # Store named entities
    resolved = []
    
    for sent in sentences:
        # Basic pronoun resolution logic
        if 'He' in sent or 'She' in sent:
            # Replace with last mentioned person
            # (This is a simplified example)
            pass
        resolved.append(sent)
    
    return '. '.join(resolved)

# Example
text = "John went to the store. He bought milk."
# Would resolve to: "John went to the store. John bought milk."
```

> **Visual Aid Suggestion**: A flowchart showing the preprocessing pipeline would help visualize the sequential nature of these operations and their dependencies.

---

## Step 3: Feature Engineering

### **Core Concept: Text Vectorization**

**The Challenge**: Machine learning algorithms work with numbers, not text. Feature engineering converts text into numerical representations that preserve semantic meaning.

### **Two Primary Approaches**

#### **Machine Learning Approach**
- **Manual Feature Creation**: Domain knowledge required
- **Advantages**: 
  - Interpretable results
  - Can justify model decisions
- **Disadvantages**: 
  - Requires domain expertise
  - Time-intensive process
  - Some features may hurt performance

#### **Deep Learning Approach**
- **Automatic Feature Generation**: Neural networks create features
- **Advantages**: 
  - No domain knowledge required
  - Handles complex patterns automatically
- **Disadvantages**: 
  - Black box (low interpretability)
  - Requires large datasets

### **Example: Sentiment Analysis Feature Engineering**

**Manual Approach**: Create features based on domain knowledge
```
Original Data:
Review Text | Sentiment
"Great movie, loved it!" | Positive
"Terrible acting, waste of time" | Negative

Feature Engineering:
Positive Words | Negative Words | Neutral Words | Exclamation | Length | Sentiment
2              | 0              | 1             | 1          | 4      | Positive
0              | 3              | 2             | 0          | 5      | Negative
```

```python
def extract_sentiment_features(text):
    """
    Extract manual features for sentiment analysis
    """
    # Sentiment lexicons
    positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 
                     'fantastic', 'love', 'best', 'happy'}
    negative_words = {'bad', 'terrible', 'awful', 'horrible', 'worst', 
                     'hate', 'disappointing', 'poor'}
    
    # Convert to lowercase and tokenize
    words = text.lower().split()
    
    # Feature extraction
    features = {
        'positive_count': sum(1 for w in words if w in positive_words),
        'negative_count': sum(1 for w in words if w in negative_words),
        'exclamation_count': text.count('!'),
        'question_count': text.count('?'),
        'uppercase_ratio': sum(1 for c in text if c.isupper()) / len(text),
        'text_length': len(words),
        'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0
    }
    
    return features

# Example usage
text = "This movie is AMAZING! Best film ever!"
features = extract_sentiment_features(text)
print(features)
```

**Advanced Techniques Preview**:
- **Bag of Words (BoW)**: Word frequency vectors
- **TF-IDF**: Term frequency-inverse document frequency
- **Word Embeddings**: Dense vector representations (Word2Vec, GloVe)
- **N-grams**: Capturing word sequences
- **Character-level Features**: For handling misspellings and OOV words
- **Contextual Embeddings**: BERT, RoBERTa, GPT embeddings

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import numpy as np

# Bag of Words Example
def create_bow_features(texts):
    vectorizer = CountVectorizer(max_features=1000)
    bow_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    return bow_matrix, feature_names

# TF-IDF Example
def create_tfidf_features(texts):
    vectorizer = TfidfVectorizer(max_features=1000, 
                                ngram_range=(1, 2),  # Unigrams and bigrams
                                min_df=2,            # Minimum document frequency
                                max_df=0.8)          # Maximum document frequency
    tfidf_matrix = vectorizer.fit_transform(texts)
    return tfidf_matrix, vectorizer

# N-gram Features
def create_ngram_features(texts, n=2):
    vectorizer = CountVectorizer(ngram_range=(n, n))
    ngram_matrix = vectorizer.fit_transform(texts)
    return ngram_matrix, vectorizer.get_feature_names_out()
```

---

## Step 4: Modeling

### **Four Algorithmic Approaches**

#### **1. Heuristic/Rule-Based Methods**
- **When to Use**: Very limited data (< 100 samples)
- **Examples**: 
  - Email spam detection by sender domain
  - Sentiment analysis using keyword lists
- **Hybrid Approach**: Combine rules with ML features

#### **2. Machine Learning Algorithms**
- **When to Use**: Moderate data availability (100s to 10,000s samples)
- **Common Algorithms**: Naive Bayes, SVM, Random Forest
- **Advantage**: Interpretable results

#### **3. Deep Learning Approaches**
- **When to Use**: Large datasets (10,000+ samples)
- **Includes**: Transfer learning with pre-trained models (BERT, GPT)
- **Advantage**: Automatic feature learning

#### **4. Cloud APIs**
- **When to Use**: Existing solutions available and budget permits
- **Examples**: Google Cloud NLP, AWS Comprehend, Azure Text Analytics
- **Advantage**: Ready-to-use, no development needed

### **Selection Criteria**

**Data Volume Decision Tree**:
- **< 100 samples**: Heuristic approaches
- **100-10K samples**: Machine Learning + some heuristics
- **10K+ samples**: Deep Learning or Transfer Learning
- **Any volume + budget**: Cloud APIs

**Detailed Decision Framework**:

```python
def select_modeling_approach(data_size, task_type, resources):
    """
    Recommend modeling approach based on constraints
    """
    if resources.get('cloud_budget') and resources.get('low_latency'):
        return 'Cloud APIs'
    
    if data_size < 100:
        return 'Rule-based + Simple ML'
    elif data_size < 1000:
        if task_type in ['classification', 'sentiment']:
            return 'Traditional ML (Naive Bayes, SVM)'
        else:
            return 'Pre-trained models with fine-tuning'
    elif data_size < 10000:
        return 'Ensemble ML or Small Neural Networks'
    else:
        if resources.get('gpu_available'):
            return 'Deep Learning (LSTM, Transformer)'
        else:
            return 'Transfer Learning with frozen embeddings'

# Algorithm Complexity vs Performance Trade-offs
ALGORITHM_COMPARISON = {
    'Naive Bayes': {'complexity': 'Low', 'interpretability': 'High', 'data_needed': 'Low'},
    'SVM': {'complexity': 'Medium', 'interpretability': 'Medium', 'data_needed': 'Medium'},
    'Random Forest': {'complexity': 'Medium', 'interpretability': 'Medium', 'data_needed': 'Medium'},
    'LSTM': {'complexity': 'High', 'interpretability': 'Low', 'data_needed': 'High'},
    'BERT': {'complexity': 'Very High', 'interpretability': 'Low', 'data_needed': 'Medium'}
}
```

**Problem Nature Considerations**:
- **Standard tasks** (sentiment, classification): Cloud APIs or transfer learning
- **Domain-specific tasks**: Custom ML/DL approaches
- **Real-time requirements**: Lightweight ML models
- **High accuracy needs**: Deep learning with large datasets

---

## Step 5: Evaluation

### **Two Evaluation Paradigms**

#### **Intrinsic Evaluation (Technical Assessment)**
- **Focus**: Model performance on technical metrics
- **Examples**: 
  - Accuracy, Precision, Recall, F1-score
  - Perplexity (for language models)
  - BLEU scores (for translation)

#### **Extrinsic Evaluation (Business Assessment)**
- **Focus**: Real-world business impact
- **Examples**:
  - User engagement with suggestions
  - Click-through rates
  - Task completion rates

### **Case Study: Text Generation Keyboard**

**Intrinsic Metric**: Perplexity score measuring model confidence
```
Input: "I had such"
Suggestions: ["great", "fun", "time"]
Perplexity: 15.2 (lower is better)
```

**Extrinsic Metric**: User acceptance rate
```
Suggestions Shown: 1000
User Selections: 450
Acceptance Rate: 45%
```

**Comprehensive Evaluation Framework**:

```python
class NLPEvaluator:
    def __init__(self, task_type):
        self.task_type = task_type
        self.intrinsic_metrics = []
        self.extrinsic_metrics = []
    
    def evaluate_intrinsic(self, predictions, ground_truth):
        """Calculate technical performance metrics"""
        if self.task_type == 'classification':
            from sklearn.metrics import accuracy_score, precision_recall_fscore_support
            
            accuracy = accuracy_score(ground_truth, predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(
                ground_truth, predictions, average='weighted'
            )
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
        
        elif self.task_type == 'generation':
            # Calculate perplexity, BLEU scores
            pass
    
    def evaluate_extrinsic(self, user_interactions):
        """Calculate business impact metrics"""
        return {
            'user_satisfaction': user_interactions['satisfied'] / user_interactions['total'],
            'task_completion_rate': user_interactions['completed'] / user_interactions['started'],
            'average_time_saved': user_interactions['time_saved_seconds']
        }

# Metrics by Task Type
TASK_METRICS = {
    'Classification': ['Accuracy', 'F1-Score', 'Confusion Matrix'],
    'NER': ['Entity-level F1', 'Token-level Accuracy'],
    'Translation': ['BLEU', 'METEOR', 'Human Evaluation'],
    'Summarization': ['ROUGE', 'BERTScore', 'Factual Consistency'],
    'Question Answering': ['Exact Match', 'F1-Score', 'MRR']
}
```

> **Critical Insight**: Good intrinsic scores don't guarantee good extrinsic performance, but good extrinsic performance usually implies good intrinsic scores.

---

## Step 6: Deployment

### **Three Deployment Phases**

#### **Phase 1: Initial Deployment**
**Deployment Strategy Depends on Application Type**:

**Microservice Deployment** (Email Spam Filter):
- Deploy model as API endpoint
- Integration with existing email system
- Input: Email text â†’ Output: Spam/Not Spam classification

**Full Application Deployment** (Chatbot):
- WhatsApp/Telegram bot integration
- Custom Android/iOS application
- Web-based chat interface

#### **Phase 2: Monitoring**
**Key Monitoring Components**:
- **Dashboard Creation**: Real-time metric visualization
- **Historical Tracking**: Performance trends over time
- **Alerting System**: Automated notifications for performance drops

**Monitored Metrics**:
- Intrinsic performance metrics
- Extrinsic business metrics
- System performance (latency, throughput)
- Data drift detection

```python
class ModelMonitor:
    def __init__(self, model_name, baseline_metrics):
        self.model_name = model_name
        self.baseline_metrics = baseline_metrics
        self.alert_thresholds = {
            'accuracy_drop': 0.05,  # 5% drop triggers alert
            'latency_increase': 0.2,  # 20% increase triggers alert
            'error_rate': 0.01  # 1% error rate triggers alert
        }
    
    def monitor_performance(self, current_metrics):
        alerts = []
        
        # Check accuracy degradation
        if (self.baseline_metrics['accuracy'] - current_metrics['accuracy'] > 
            self.alert_thresholds['accuracy_drop']):
            alerts.append('Accuracy degradation detected')
        
        # Check latency
        if (current_metrics['latency'] / self.baseline_metrics['latency'] > 
            1 + self.alert_thresholds['latency_increase']):
            alerts.append('Latency increase detected')
        
        return alerts
    
    def detect_data_drift(self, new_data_distribution, original_distribution):
        """Detect if input data distribution has changed"""
        from scipy.stats import ks_2samp
        
        # Kolmogorov-Smirnov test for distribution comparison
        statistic, p_value = ks_2samp(new_data_distribution, original_distribution)
        
        if p_value < 0.05:  # Significant difference
            return True, f"Data drift detected (p-value: {p_value:.4f})"
        return False, "No significant drift"
```

#### **Phase 3: Model Updates**
**Update Triggers**:
- **Business Expansion**: New markets, languages, or use cases
- **Performance Degradation**: Model drift over time
- **Data Evolution**: Changing user behavior patterns

**Update Strategies**:
- **Batch Updates**: Periodic retraining with new data
- **Online Learning**: Continuous model adaptation
- **A/B Testing**: Gradual rollout of updated models

---

## Pipeline Characteristics

### **Non-Linear Nature**
The NLP pipeline is **iterative and cyclical**:
- Deployment results may require returning to feature engineering
- Model performance might necessitate additional preprocessing
- New data may demand algorithm changes

### **Problem-Specific Adaptations**
- **Sentiment Analysis**: Focus on emotion-bearing words
- **Text Summarization**: Emphasize sentence importance scoring
- **Machine Translation**: Require sequence-to-sequence modeling
- **Question Answering**: Need context-answer relationship modeling

---

## Practical Assignment: Quora Question Similarity Detection

### **Problem Statement**
Build a system to identify duplicate questions on Quora that have the same meaning but different wording.

**Example Duplicate Questions**:
- "Is IPL destroying Indian cricket?"
- "Is IPL spoiling Indian cricket?"

### **Implementation Approach**

```python
class QuoraDuplicateDetector:
    def __init__(self):
        self.preprocessing_pipeline = []
        self.feature_extractors = []
        self.model = None
    
    def preprocess(self, question1, question2):
        """Apply preprocessing to question pairs"""
        # 1. Basic cleaning
        q1_clean = self.clean_text(question1)
        q2_clean = self.clean_text(question2)
        
        # 2. Tokenization
        q1_tokens = self.tokenize(q1_clean)
        q2_tokens = self.tokenize(q2_clean)
        
        return q1_tokens, q2_tokens
    
    def extract_features(self, q1_tokens, q2_tokens):
        """Extract similarity features"""
        features = {}
        
        # 1. Length-based features
        features['len_diff'] = abs(len(q1_tokens) - len(q2_tokens))
        features['len_ratio'] = len(q1_tokens) / max(len(q2_tokens), 1)
        
        # 2. Word overlap features
        common_words = set(q1_tokens) & set(q2_tokens)
        features['common_word_ratio'] = len(common_words) / max(len(set(q1_tokens)), len(set(q2_tokens)))
        
        # 3. N-gram overlap
        q1_bigrams = set(zip(q1_tokens[:-1], q1_tokens[1:]))
        q2_bigrams = set(zip(q2_tokens[:-1], q2_tokens[1:]))
        features['bigram_overlap'] = len(q1_bigrams & q2_bigrams) / max(len(q1_bigrams), len(q2_bigrams), 1)
        
        # 4. Semantic similarity (using pre-trained embeddings)
        features['cosine_similarity'] = self.compute_semantic_similarity(q1_tokens, q2_tokens)
        
        return features
    
    def compute_semantic_similarity(self, tokens1, tokens2):
        """Compute semantic similarity using embeddings"""
        # Placeholder for embedding-based similarity
        # In practice, use Word2Vec, GloVe, or BERT embeddings
        pass

# Feature Ideas for Quora Dataset
FEATURE_IDEAS = [
    'Word overlap ratio',
    'Character n-gram similarity',
    'TF-IDF cosine similarity',
    'Word2Vec/GloVe average similarity',
    'Levenshtein distance',
    'Jaccard similarity',
    'Common named entities',
    'Question type matching (what/how/why)',
    'BERT sentence embeddings cosine similarity'
]
```

### **Assignment Framework**

**1. Data Acquisition Strategy**
- Where will you source labeled question pairs?
- How will you handle the supervised learning requirement?

**2. Text Preprocessing Plan**
- What cleaning steps are necessary for question text?
- Which basic and advanced preprocessing techniques apply?

**3. Feature Engineering Approach**
- What features best capture question similarity?
- How will you represent semantic meaning numerically?

**4. Modeling Strategy**
- Which algorithms suit similarity detection?
- How will you handle the binary classification task?

**5. Evaluation Metrics**
- What intrinsic metrics measure similarity accuracy?
- How will you assess business impact for Quora?

**6. Integration and Monitoring**
- How will this integrate with Quora's existing system?
- What metrics require ongoing monitoring?

---

## Key Takeaways

1. **Systematic Approach**: The 5-step pipeline provides a structured methodology for any NLP project

2. **Iterative Process**: Expect to cycle through steps multiple times to optimize performance

3. **Data-Driven Decisions**: The amount and quality of data fundamentally determine your approach options

4. **Business Context Matters**: Technical performance must align with business objectives

5. **Scalability Considerations**: Design solutions that can evolve with growing data and changing requirements

6. **Tool Selection**: Choose appropriate tools based on:
   - Data volume and velocity
   - Accuracy requirements
   - Latency constraints
   - Budget limitations
   - Team expertise

7. **Continuous Learning**: NLP is rapidly evolving - stay updated with:
   - New pre-trained models (GPT, BERT variants)
   - Efficient fine-tuning techniques
   - Domain adaptation methods
   - Multilingual capabilities

## Reflection Questions

- **How might the pipeline steps change for real-time vs. batch processing requirements?**
- **What trade-offs exist between model interpretability and performance in different business contexts?**
- **When should you prioritize rule-based approaches over ML/DL solutions?**
- **How do you handle multilingual requirements in the pipeline?**
- **What are the ethical considerations at each pipeline stage?**

## Practical Exercises

1. **Mini Project 1**: Build a simple spam classifier following all 5 pipeline steps
2. **Mini Project 2**: Create a named entity recognition system for your domain
3. **Mini Project 3**: Implement a basic chatbot using rule-based + ML hybrid approach

## Resources for Deep Dive

### **Books**
- "Speech and Language Processing" by Jurafsky & Martin
- "Natural Language Processing with Python" by Bird, Klein & Loper

### **Online Courses**
- Stanford CS224N: Natural Language Processing with Deep Learning
- Fast.ai NLP Course

### **Tools & Libraries**
- **Preprocessing**: NLTK, spaCy, TextBlob
- **Feature Engineering**: scikit-learn, Gensim
- **Deep Learning**: Transformers (Hugging Face), PyTorch, TensorFlow
- **Cloud APIs**: Google Cloud NLP, AWS Comprehend, Azure Text Analytics

### **Datasets for Practice**
- IMDB Movie Reviews (Sentiment Analysis)
- CoNLL-2003 (Named Entity Recognition)
- SQuAD (Question Answering)
- Multi30K (Machine Translation)

**Next Steps**: Practice applying this pipeline framework to various NLP problems, starting with the Quora assignment to solidify understanding of each step's practical implementation.

[End of Notes]